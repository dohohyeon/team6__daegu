import requests
import numpy as np
import pandas as pd
import geopandas as gpd
import time
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from rapidfuzz import fuzz
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from folium import Map, CircleMarker, LayerControl, FeatureGroup
import folium
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd, re, unicodedata

df1 = pd.read_csv("data/2020.csv")
df2 = pd.read_csv("data/2022.csv")
df2['ì‚¬ê³ ê±´ìˆ˜'] = df1['ì–´ë¦°ì´ë³´í–‰ì ì‚¬ê³ ê±´ìˆ˜ (2020ë…„ ~ 2021ë…„)'] + df2['ì–´ë¦°ì´ë³´í–‰ì ì‚¬ê³ ê±´ìˆ˜ (2022ë…„ ~ 2024ë…„)']

del df2['ì–´ë¦°ì´ë³´í–‰ì ì‚¬ê³ ê±´ìˆ˜ (2022ë…„ ~ 2024ë…„)']
df2

df3 = pd.read_csv("data/ëŒ€êµ¬ì–´ë¦°ì´ë³´í˜¸êµ¬ì—­.csv",encoding='cp949')

df2.head()
df3.head()


# df2 df3 ì¡°ì¸

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def clean_text(x):
    if pd.isna(x):
        return ""
    return str(x).replace(" ", "").strip().lower()

# ì „ì²˜ë¦¬
df2['ì‹œì„¤ëª…_clean'] = df2['ì‹œì„¤ëª…'].apply(clean_text)
df2['ì£¼ì†Œ_clean'] = df2['ì£¼ì†Œ'].apply(clean_text)
df3['ëŒ€ìƒì‹œì„¤ëª…_clean'] = df3['ëŒ€ìƒì‹œì„¤ëª…'].apply(clean_text)
df3['ì†Œì¬ì§€ë„ë¡œëª…ì£¼ì†Œ_clean'] = df3['ì†Œì¬ì§€ë„ë¡œëª…ì£¼ì†Œ'].apply(clean_text)

matches = []

for i, (name2, addr2) in enumerate(zip(df2['ì‹œì„¤ëª…_clean'], df2['ì£¼ì†Œ_clean'])):
    best_score = 0
    best_idx = None
    
    for j, (name3, addr3) in enumerate(zip(df3['ëŒ€ìƒì‹œì„¤ëª…_clean'], df3['ì†Œì¬ì§€ë„ë¡œëª…ì£¼ì†Œ_clean'])):
        score_name = fuzz.token_sort_ratio(name2, name3)
        score_addr = fuzz.token_sort_ratio(addr2, addr3)
        
        # ì´ë¦„ ë˜ëŠ” ì£¼ì†Œ ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ 95 ì´ìƒì´ë©´ ë§¤ì¹­ í›„ë³´
        if score_name >= 95 or score_addr >= 95:
            combined_score = max(score_name, score_addr)  # ë†’ì€ ìª½ ì ìˆ˜ ì‚¬ìš©
            if combined_score > best_score:
                best_score = combined_score
                best_idx = j
    
    if best_idx is not None:
        matches.append((best_score, best_idx))
    else:
        matches.append((None, None))

matches_df = pd.DataFrame(matches, columns=['ìœ ì‚¬ë„', 'df3_idx'])

df2_matched = pd.concat([df2.reset_index(drop=True), matches_df], axis=1)

result = df2_matched.merge(
    df3.reset_index().rename(columns={'index': 'df3_idx'}),
    on='df3_idx',
    how='left'
)

print(result.head())
result.to_csv("ê²°í•©3.csv", index=False, encoding='utf-8-sig')

# ì´ë¦„,ì£¼ì†Œ ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì¡°ì¸ í›„, ë§¤ì¹­ ì•ˆ ëœ ë°ì´í„°ëŠ” ìˆ˜ì‘ì—…ìœ¼ë¡œ ì§„í–‰


df = pd.read_csv("data/dataset.csv", encoding='cp949')
df.columns

# ìœ„ë„, ê²½ë„ ì •ë³´ë¡œ ìë©´ë™ ë³€ìˆ˜ ìƒì„±

# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„

# ì¹´ì¹´ì˜¤ REST API í‚¤ (ë³¸ì¸ ë°œê¸‰ í‚¤ë¡œ êµì²´)
KAKAO_API_KEY = "1b7f9f50feea28d092529d889942308d"

def get_address(lat, lon):
    url = "https://dapi.kakao.com/v2/local/geo/coord2address.json"
    headers = {
        "Authorization": f"KakaoAK {KAKAO_API_KEY}"
    }
    params = {
        "x": lon,  # ê²½ë„
        "y": lat   # ìœ„ë„
    }
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            data = response.json()
            try:
                # í–‰ì •ë™ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                return data['documents'][0]['address']['region_3depth_name']
            except (IndexError, KeyError):
                return None
        else:
            print(f"API ì˜¤ë¥˜ {response.status_code}: {response.text}")
            return None
    except Exception as e:
        print(f"ì˜ˆì™¸ ë°œìƒ: {e}")
        return None

# ë°ì´í„°í”„ë ˆì„ì— ì ìš©
df['ìë©´ë™'] = [get_address(lat, lon) for lat, lon in zip(df['ìœ„ë„'], df['ê²½ë„'])]
time.sleep(0.2)  # API í˜¸ì¶œ ì œí•œ ê³ ë ¤

print(df)
len(df['ìë©´ë™'].unique())

df.columns
df.to_csv("dataset1.csv", index=False, encoding='utf-8-sig')

# ë™ì´ë¦„ ë³€ê²½ëœ ê±°ëŠ” ìˆ˜ì‘ì—…ìœ¼ë¡œ ë³€ê²½í•¨

dataset2 = pd.read_csv("data/dataset2.csv", encoding='cp949')
speed = pd.read_csv("data/speed.csv", encoding='cp949')

# ê¸°ì¡´ ë°ì´í„°ì…‹ì— ì£¼í–‰ì†ë„ ë°ì´í„° ì¡°ì¸

# ì˜ˆì‹œ: dataset2ì™€ speed ëª¨ë‘ 'ìë©´ë™' ì»¬ëŸ¼ì„ ê°€ì§€ê³  ìˆë‹¤ê³  ê°€ì •
result = pd.merge(
    dataset2,
    speed,
    on='ìë©´ë™',        # ì¡°ì¸ í‚¤
    how='left'         # left outer join
)

result[result['í‰ê· ì£¼í–‰ì†ë„'].isnull()]
result.to_csv("dataset4.csv", index=False, encoding='utf-8-sig')



# ì–´ë¦°ì´ ì¸êµ¬ìˆ˜ ì¡°ì¸

df1 = pd.read_csv("data/dataset10.csv", encoding='utf-8')
df2 = pd.read_csv("data/dataset2.csv", encoding='cp949')



df1.info()
df2.info()

df_merged = pd.merge(
    df1, df2[['ìœ„ë„','ê²½ë„','ìë©´ë™']], 
    on=['ìœ„ë„','ê²½ë„'], 
    how='left'
)

df_merged.to_csv("dataset11.csv", index=False, encoding="utf-8-sig")


df1 = pd.read_csv("data/dataset11.csv", encoding='utf-8-sig')
df2 = pd.read_csv("data/ì–´ë¦°ì´ì¸êµ¬.csv", encoding='utf-8-sig')

df_merged = pd.merge(
    df1, df2, 
    on='ìë©´ë™', 
    how='left'
)

df_merged.to_csv("dataset12.csv", index=False, encoding="utf-8-sig")
df_merged.to_csv("dataset13.csv", index=False, encoding="cp949")

df2.to_csv("ì–´ë¦°ì´ì¸êµ¬1.csv", index=False, encoding="utf-8-sig")

df = pd.read_csv("data/dataset.csv", encoding='utf-8-sig')
df.info()
df.to_csv("dataset1.csv", index=False, encoding="utf-8-sig")
df.to_csv("dataset2.csv", index=False, encoding="cp949")


# ê²°í•© ë ë¶„ì„ ì‹œì‘
df = pd.read_csv("data/dataset1.csv", encoding='utf-8-sig')
df.info()

col_accidents   = 'ì‚¬ê³ ê±´ìˆ˜'
col_speed       = 'í‰ê· ì£¼í–‰ì†ë„'
col_illegal     = 'ë¶ˆë²•ì£¼ì •ì°¨ìœ„ë°˜ê±´ìˆ˜'
col_cctv        = 'ì‹œì„¤ë¬¼ CCTV ìˆ˜'
col_sign        = 'ì‹œì„¤ë¬¼ ë„ë¡œí‘œì§€íŒ ìˆ˜'
col_bump        = 'ì‹œì„¤ë¬¼ ê³¼ì†ë°©ì§€í„± ìˆ˜'
col_width       = 'ë³´í˜¸êµ¬ì—­ë„ë¡œí­'
col_signals     = 'ì‹ í˜¸ë“±_150m_ê°œìˆ˜'


# ë¶„ì„ ì‹œì‘
# EDA

df = pd.read_csv("data/dataset1.csv", encoding='utf-8')
df.to_csv("dataset1.csv", index=False, encoding='utf-8-sig')

df.info()

df['ì „ì²´ì¸êµ¬'] = df['ì „ì²´ì¸êµ¬'].str.replace(',', '', regex=False).astype(int)
df['ì–´ë¦°ì´ì¸êµ¬'] = df['ì–´ë¦°ì´ì¸êµ¬'].str.replace(',', '', regex=False).astype(int)
df['ì–´ë¦°ì´ë¹„ìœ¨'] = (df['ì–´ë¦°ì´ì¸êµ¬'] / df['ì „ì²´ì¸êµ¬'])*100
df['ì–´ë¦°ì´ë¹„ìœ¨'] = df['ì–´ë¦°ì´ë¹„ìœ¨'].round(2)
df['ì–´ë¦°ì´ë¹„ìœ¨'].describe()
df.info()

del df['ìœ„í—˜ë„ì ìˆ˜']
del df['RawScore']
del df['ì‹ í˜¸ë“±_150m_ê°œìˆ˜']

df.info()

risk_vars = ['ì‚¬ê³ ê±´ìˆ˜', 'í‰ê· ì£¼í–‰ì†ë„', 'ë¶ˆë²•ì£¼ì •ì°¨ìœ„ë°˜ê±´ìˆ˜', 'ì–´ë¦°ì´ë¹„ìœ¨', 'ë³´í˜¸êµ¬ì—­ë„ë¡œí­']
protect_vars = ['ì‹œì„¤ë¬¼ CCTV ìˆ˜', 'ì‹œì„¤ë¬¼ ë„ë¡œí‘œì§€íŒ ìˆ˜', 'ì‹œì„¤ë¬¼ ê³¼ì†ë°©ì§€í„± ìˆ˜', 'ì‹ í˜¸ë“± 300m']

# 2) Min-Max ìŠ¤ì¼€ì¼ë§ (0~1)
all_vars = risk_vars + protect_vars
scaler = MinMaxScaler()
scaled = pd.DataFrame(scaler.fit_transform(df[all_vars]), columns=all_vars, index=df.index)

# 3) ë³´í˜¸ ìš”ì¸ì€ ë°©í–¥ ë’¤ì§‘ê¸° (ê°’â†‘ â†’ ìœ„í—˜â†“ â†’ ë°˜ëŒ€ë¡œ ê³„ì‚°í•´ì•¼ í•¨)
for c in protect_vars:
    scaled[c] = 1 - scaled[c]

# 4) ê°€ì¤‘ì¹˜ ì„¤ì • (ì‚¬ìš©ìê°€ ì§€ì •í•œ ê°’)
weights = {
    'ì‚¬ê³ ê±´ìˆ˜': 0.3,
    'í‰ê· ì£¼í–‰ì†ë„': 0.13,
    'ë¶ˆë²•ì£¼ì •ì°¨ìœ„ë°˜ê±´ìˆ˜': 0.10,
    'ì–´ë¦°ì´ë¹„ìœ¨': 0.07,
    'ë³´í˜¸êµ¬ì—­ë„ë¡œí­': 0.05,
    'ì‹œì„¤ë¬¼ CCTV ìˆ˜': 0.10,
    'ì‹œì„¤ë¬¼ ë„ë¡œí‘œì§€íŒ ìˆ˜': 0.05,
    'ì‹œì„¤ë¬¼ ê³¼ì†ë°©ì§€í„± ìˆ˜': 0.10,
    'ì‹ í˜¸ë“± 300m': 0.10
}

# 5) ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘í•© â†’ 0~100 ì ìˆ˜í™”)
df['ìœ„í—˜ë„ì ìˆ˜'] = sum(scaled[col] * w for col, w in weights.items())
df['ìœ„í—˜ë„ì ìˆ˜'] = df['ìœ„í—˜ë„ì ìˆ˜'] / sum(weights.values()) * 100
scaler = MinMaxScaler(feature_range=(0,100))
df['ìœ„í—˜ë„ì ìˆ˜_norm'] = scaler.fit_transform(df[['ìœ„í—˜ë„ì ìˆ˜']])
print(df['ìœ„í—˜ë„ì ìˆ˜_norm'].describe())

del df['ìœ„í—˜ë„ì ìˆ˜_norm']

df.to_csv("dataset.csv", index=False, encoding='utf-8-sig')

df['ìœ„í—˜ë„ì ìˆ˜'].describe()

np.sum(df['ìœ„í—˜ë„ì ìˆ˜'] >= 46)

df['ìœ„í—˜ë„ì ìˆ˜'].hist()
# 6) ê²°ê³¼ í™•ì¸
print(df[['ì‹œì„¤ëª…','ì‚¬ê³ ê±´ìˆ˜','ìœ„í—˜ë„ì ìˆ˜']].head())




plt.scatter(df['ìœ„í—˜ë„ì ìˆ˜_ë…¸ì‚¬ê³ '], df['ì‚¬ê³ ê±´ìˆ˜'])
plt.scatter(df['ìœ„í—˜ë„ì ìˆ˜'], df['ì‚¬ê³ ê±´ìˆ˜'])



# ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
numeric_cols = df.select_dtypes(include=['float64', 'int64'])

# ìƒê´€ê³„ìˆ˜ ê³„ì‚° (ìŠ¤í”¼ì–´ë§Œ)
corr_matrix = numeric_cols.corr(method='spearman')
print(corr_matrix)


plt.rc('font', family='Malgun Gothic')  # Windows: ë§‘ì€ ê³ ë”•
plt.rc('axes', unicode_minus=False)     # ìŒìˆ˜ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

# ---- (2) ìƒê´€ê³„ìˆ˜ ê³„ì‚° ----
numeric_cols = df.select_dtypes(include=['float64', 'int64'])
corr_matrix = numeric_cols.corr(method='spearman')

# ---- (3) íˆíŠ¸ë§µ ì‹œê°í™” ----
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix,
            annot=True,       # ìƒê´€ê³„ìˆ˜ ìˆ«ì í‘œì‹œ
            fmt=".2f",        # ì†Œìˆ˜ì  2ìë¦¬
            cmap='coolwarm',  # ìƒ‰ìƒ
            cbar=True,
            square=True)

plt.title('ë³€ìˆ˜ ê°„ ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µ', fontsize=14)
plt.show()


df.hist(figsize=(12, 10), bins=20, grid=False)
plt.tight_layout()
plt.show()

df.info()


df = pd.read_csv("data/dataset.csv", encoding='utf-8')
pat = re.compile(r'(ë‹¬ì„œ\s*êµ¬|ë‹¬ì„±\s*êµ°|ìˆ˜ì„±\s*êµ¬|ë¶\s*êµ¬|ë‚¨\s*êµ¬|ë™\s*êµ¬|ì„œ\s*êµ¬|ì¤‘\s*êµ¬)\b')

def normalize(s):
    if not isinstance(s, str): return ""
    s = unicodedata.normalize("NFKC", s).replace("\u00A0"," ").replace("\u3000"," ")
    return re.sub(r"\s+", " ", s).strip()

pat = re.compile(r'(ë‹¬ì„œ\s*êµ¬|ë‹¬ì„±\s*êµ°|ìˆ˜ì„±\s*êµ¬|ë¶\s*êµ¬|ë‚¨\s*êµ¬|ë™\s*êµ¬|ì„œ\s*êµ¬|ì¤‘\s*êµ¬)\b')

df["êµ¬êµ°"] = (
    df["ì£¼ì†Œ"].astype(str).map(normalize)
      .str.extract(pat, expand=False)
      .str.replace(r"\s+", "", regex=True)
      .astype(str).str.strip()
)
df.to_csv("ê²°í•©2.csv", index=False, encoding='utf-8-sig')
# êµ°ì§‘ë¶„ì„
df.info()

feature_cols = ['ì‚¬ê³ ê±´ìˆ˜', 
    'í‰ê· ì£¼í–‰ì†ë„', 'ë¶ˆë²•ì£¼ì •ì°¨ìœ„ë°˜ê±´ìˆ˜', 'ì–´ë¦°ì´ë¹„ìœ¨',
    'ì‹œì„¤ë¬¼ CCTV ìˆ˜', 'ì‹œì„¤ë¬¼ ë„ë¡œí‘œì§€íŒ ìˆ˜', 'ì‹œì„¤ë¬¼ ê³¼ì†ë°©ì§€í„± ìˆ˜',
    'ë³´í˜¸êµ¬ì—­ë„ë¡œí­', 'ì‹ í˜¸ë“± 300m', 'ìœ„ë„', 'ê²½ë„' # í•„ìš” ì—†ìœ¼ë©´ ë¹¼ë„ ë©ë‹ˆë‹¤.
]
X = df[feature_cols].copy()

'''
# (ì„ íƒ) ì¹´ìš´íŠ¸í˜•ì˜ ì™œë„ ì™„í™”: log1p
log_candidates = ['ë¶ˆë²•ì£¼ì •ì°¨ìœ„ë°˜ê±´ìˆ˜', 'ì‹œì„¤ë¬¼ CCTV ìˆ˜', 'ì‹œì„¤ë¬¼ ë„ë¡œí‘œì§€íŒ ìˆ˜',
                  'ì‹œì„¤ë¬¼ ê³¼ì†ë°©ì§€í„± ìˆ˜', 'ì‹ í˜¸ë“±_150m_ê°œìˆ˜', 'êµ¬ì—­ì§€ì •ìˆ˜']
for c in feature_cols:
    if c in log_candidates:
        X[c] = np.log1p(X[c].clip(lower=0))
'''
# 3) í‘œì¤€í™”
scaler = StandardScaler()
Xs = scaler.fit_transform(X)

'''
# 4) k í›„ë³´ì— ëŒ€í•´ ì‹¤ë£¨ì—£ ê³„ìˆ˜ ê³„ì‚°
k_range = range(2, 9)  # k=2~8 ì‹œí—˜
sil_scores = {}
sse = {}  # ì—˜ë³´ìš° ì°¸ê³ ìš©

for k in k_range:
    km = KMeans(n_clusters=k, n_init=30, random_state=42)
    labels = km.fit_predict(Xs)
    sil = silhouette_score(Xs, labels)
    sil_scores[k] = sil
    sse[k] = km.inertia_

print("Silhouette scores by k:", sil_scores)
print("SSE (inertia) by k:", sse)

# 5) ì‹¤ë£¨ì—£ ìµœê³  k ì„ íƒ
best_k = max(sil_scores, key=sil_scores.get)
print("Selected k (by silhouette):", best_k)
'''

# 6) ìµœì¢… KMeans í•™ìŠµ ë° ë¼ë²¨ ë¶€ì—¬
kmeans = KMeans(n_clusters=5, n_init=50, random_state=42)
df['cluster'] = kmeans.fit_predict(Xs)
df.info()

group_mean = df.groupby('cluster')[['ì‚¬ê³ ê±´ìˆ˜',
    'í‰ê· ì£¼í–‰ì†ë„', 'ë¶ˆë²•ì£¼ì •ì°¨ìœ„ë°˜ê±´ìˆ˜', 'ì–´ë¦°ì´ë¹„ìœ¨',
    'ì‹œì„¤ë¬¼ CCTV ìˆ˜', 'ì‹œì„¤ë¬¼ ë„ë¡œí‘œì§€íŒ ìˆ˜', 'ì‹œì„¤ë¬¼ ê³¼ì†ë°©ì§€í„± ìˆ˜',
    'ë³´í˜¸êµ¬ì—­ë„ë¡œí­', 'ì‹ í˜¸ë“± 300m',
    'ìœ„í—˜ë„ì ìˆ˜'
]].mean().round(2)
print(group_mean)

group_stats = df.groupby("cluster")[[
    "ì‚¬ê³ ê±´ìˆ˜", "í‰ê· ì£¼í–‰ì†ë„", "ë¶ˆë²•ì£¼ì •ì°¨ìœ„ë°˜ê±´ìˆ˜", "ì–´ë¦°ì´ë¹„ìœ¨",
    "ì‹œì„¤ë¬¼ CCTV ìˆ˜", "ì‹œì„¤ë¬¼ ë„ë¡œí‘œì§€íŒ ìˆ˜", "ì‹œì„¤ë¬¼ ê³¼ì†ë°©ì§€í„± ìˆ˜",
    "ë³´í˜¸êµ¬ì—­ë„ë¡œí­", "ì‹ í˜¸ë“± 300m", "ìœ„í—˜ë„ì ìˆ˜"
]].describe().round(2)

print(group_stats)

plt.rcParams['font.family'] = 'Malgun Gothic'  # ìœˆë„ìš°
plt.rcParams['axes.unicode_minus'] = False     # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€

# âœ… ì›í•˜ëŠ” ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ ì§ì ‘ ì§€ì •
variables = [
    "ì‚¬ê³ ê±´ìˆ˜",
    "í‰ê· ì£¼í–‰ì†ë„",
    "ë¶ˆë²•ì£¼ì •ì°¨ìœ„ë°˜ê±´ìˆ˜",
    "ì–´ë¦°ì´ë¹„ìœ¨",
    "ì‹œì„¤ë¬¼ CCTV ìˆ˜",
    "ì‹œì„¤ë¬¼ ë„ë¡œí‘œì§€íŒ ìˆ˜",
    "ì‹œì„¤ë¬¼ ê³¼ì†ë°©ì§€í„± ìˆ˜",
    "ë³´í˜¸êµ¬ì—­ë„ë¡œí­",
    "ì‹ í˜¸ë“± 300m",
    "ìœ„í—˜ë„ì ìˆ˜"
]

# âœ… ë³€ìˆ˜ë³„ boxplot
for col in variables:
    if col not in df.columns:
        print(f"âš ï¸ {col} ì»¬ëŸ¼ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        continue
    
    plt.figure(figsize=(8, 5))
    sns.boxplot(x="cluster", y=col, data=df, palette="Set2")
    plt.title(f"Clusterë³„ {col} ë¶„í¬", fontsize=14)
    plt.xlabel("Cluster")
    plt.ylabel(col)
    plt.tight_layout()
    plt.show()



center = [df['ìœ„ë„'].mean(), df['ê²½ë„'].mean()]
m = Map(location=center, zoom_start=12, control_scale=True, tiles="CartoDB positron")


gu_geo = gpd.read_file("data/BND/BND_SIGUNGU_PG.shp")

# 2) ì¢Œí‘œê³„ë¥¼ WGS84(ê²½ìœ„ë„)ë¡œ ë³€í™˜ (Foliumì€ EPSG:4326 í•„ìš”)
if gu_geo.crs is None or gu_geo.crs.to_string().upper() not in ("EPSG:4326", "WGS84"):
    gu_geo = gu_geo.to_crs(epsg=4326)

# 3) ëŒ€êµ¬ì‹œ êµ¬Â·êµ°ë§Œ í•„í„°ë§
DAEGU_GU_LIST = ['ìˆ˜ì„±êµ¬','ë‹¬ì„œêµ¬','ë‹¬ì„±êµ°','ë™êµ¬','ë¶êµ¬','ì¤‘êµ¬','ì„œêµ¬','ë‚¨êµ¬']
gu_daegu = gu_geo[gu_geo['SIGUNGU_NM'].isin(DAEGU_GU_LIST)].copy()

# 4) (ì„ íƒ) ëŒ€êµ¬ì‹œ ì™¸ê³½ ê²½ê³„(í•˜ë‚˜ì˜ í´ë¦¬ê³¤) ìƒì„±
daegu_boundary = gu_daegu.dissolve(by=lambda x: 1)  # ëª¨ë“  êµ¬/êµ°ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
daegu_boundary['name'] = 'ëŒ€êµ¬ê´‘ì—­ì‹œ'

# 5) êµ°Â·êµ¬ ê²½ê³„ì„  ë ˆì´ì–´ (ê²€ì€ ì ì„ )
folium.GeoJson(
    gu_daegu,
    name="ëŒ€êµ¬ êµ°Â·êµ¬ ê²½ê³„",
    style_function=lambda feat: {
        "color": "black",
        "weight": 2,
        "dashArray": "5,5",
        "fillOpacity": 0
    },
    tooltip=folium.GeoJsonTooltip(fields=["SIGUNGU_NM"], aliases=["êµ°Â·êµ¬"])
).add_to(m)

# 6) ëŒ€êµ¬ ì „ì²´ ì™¸ê³½ì„ (êµµì€ ì‹¤ì„ )
folium.GeoJson(
    daegu_boundary,
    name="ëŒ€êµ¬ ì™¸ê³½",
    style_function=lambda feat: {
        "color": "#222222",
        "weight": 3,
        "fillOpacity": 0
    },
    tooltip=folium.GeoJsonTooltip(fields=["name"], aliases=["ì§€ì—­"])
).add_to(m)

for idx, row in gu_daegu.iterrows():
    centroid = row['geometry'].centroid
    folium.map.Marker(
        [centroid.y, centroid.x],
        icon=folium.DivIcon(
            html=f"""
                <div style="font-size:14px; font-weight:bold; color:#333;">
                    {row['SIGUNGU_NM']}
                </div>
            """
        )
    ).add_to(m)

# 7) ì§€ë„ ë²”ìœ„ë¥¼ ëŒ€êµ¬ ì „ì²´ë¡œ ë§ì¶¤ (ì‹œì„¤ í¬ì¸íŠ¸ë³´ë‹¤ ê²½ê³„ì— ë§ì¶”ê³  ì‹¶ì„ ë•Œ)
minx, miny, maxx, maxy = gu_daegu.total_bounds
m.fit_bounds([[miny, minx], [maxy, maxx]])

# 3) êµ°ì§‘ ë¼ë²¨ ìˆ˜ì§‘ ë° ìƒ‰ìƒ ë§¤í•‘
clusters = sorted(df['cluster'].unique())
palette = [
    "#e41a1c", "#377eb8", "#4daf4a", "#984ea3",
    "#ff7f00", "#a65628", "#f781bf", "#999999"
] * 5  # êµ°ì§‘ì´ ë§ì•„ë„ ë°˜ë³µ ì‚¬ìš©

color_map = {c: palette[i] for i, c in enumerate(clusters)}

# 4) (ì„ íƒ) ìœ„í—˜ë„ ì ìˆ˜ì— ë”°ë¼ ë§ˆì»¤ ë°˜ì§€ë¦„ì„ ê°€ë³ê²Œ ìŠ¤ì¼€ì¼ë§
def radius_by_risk(row, base=4, add=6):
    if 'ìœ„í—˜ë„ì ìˆ˜(0~100)' in row:
        r = base + add * (row['ìœ„í—˜ë„ì ìˆ˜(0~100)'] / 100.0)  # 4~10px
        return float(r)
    return 6.0

# 5) êµ°ì§‘ë³„ FeatureGroup ë ˆì´ì–´ ìƒì„± + ë§ˆì»¤ ì¶”ê°€
for c in clusters:
    fg = FeatureGroup(name=f"Cluster {c}", show=True)
    sub = df[df['cluster'] == c]
    for _, r in sub.iterrows():
        lat, lon = float(r['ìœ„ë„']), float(r['ê²½ë„'])
        color = color_map[c]
        # íŒì—…/íˆ´íŒ ë‚´ìš© êµ¬ì„±
        fac = r.get('ì‹œì„¤ëª…', '')
        addr = r.get('ì£¼ì†Œ', '')
        risk = r.get('ìœ„í—˜ë„ì ìˆ˜(0~100)', None)
        risk_txt = f"{risk:.1f}" if pd.notna(risk) else "N/A"

        popup_html = folium.Html(f"""
        <div style="font-size:13px;">
          <b>ì‹œì„¤ëª…:</b> {fac}<br>
          <b>ì£¼ì†Œ:</b> {addr}<br>
          <b>Cluster:</b> {c}<br>
          <b>ìœ„í—˜ë„ì ìˆ˜:</b> {risk_txt}
        </div>
        """, script=True)
        popup = folium.Popup(popup_html, max_width=300)

        CircleMarker(
            location=[lat, lon],
            radius=radius_by_risk(r),
            color=color,
            fill=True,
            fill_color=color,
            fill_opacity=0.75,
            weight=1,
            popup=popup,
            tooltip=f"{fac} | Cluster {c}"
        ).add_to(fg)

    fg.add_to(m)

# 6) ë²”ë¡€(ê°„ë‹¨ í…ìŠ¤íŠ¸) ì¶”ê°€
legend_html = """
<div style="
     position: fixed; bottom: 20px; left: 20px; z-index: 9999;
     background-color: white; padding: 10px 12px; border: 1px solid #ddd;
     box-shadow: 0 1px 4px rgba(0,0,0,0.2); border-radius: 6px; font-size:13px;">
  <b>Clusters</b><br>
  {}
</div>
""".format("<br>".join(
    [f'<span style="display:inline-block;width:10px;height:10px;background:{color_map[c]};margin-right:6px;"></span> Cluster {c}'
     for c in clusters]
))
m.get_root().html.add_child(folium.Element(legend_html))

df.columns

# 7) ë ˆì´ì–´ ì»¨íŠ¸ë¡¤ ë²„íŠ¼
LayerControl(collapsed=False).add_to(m)


m.save("êµ°ì§‘.html")
print("ì €ì¥ ì™„ë£Œ: clusters_map11.html  (ë¸Œë¼ìš°ì €ë¡œ ì—´ë©´ ì§€ë„ í™•ì¸ ê°€ëŠ¥)")


# íšŒê·€ë¶„ì„

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

df = pd.read_csv("data/dataset.csv", encoding='utf-8')
df.info()
df1 = df.groupby('ìë©´ë™')[['êµ¬ì—­ì§€ì •ìˆ˜', 'ì‹œì„¤ë¬¼ CCTV ìˆ˜', 'ì‹œì„¤ë¬¼ ë„ë¡œí‘œì§€íŒ ìˆ˜', 'ì‹œì„¤ë¬¼ ê³¼ì†ë°©ì§€í„± ìˆ˜',
       'ë³´í˜¸êµ¬ì—­ë„ë¡œí­', 'ì‚¬ê³ ê±´ìˆ˜', 'í‰ê· ì£¼í–‰ì†ë„', 'ë¶ˆë²•ì£¼ì •ì°¨ìœ„ë°˜ê±´ìˆ˜','ì‹ í˜¸ë“± 300m','ì–´ë¦°ì´ì¸êµ¬','ì „ì²´ì¸êµ¬']].mean()
del df1['ì–´ë¦°ì´ë¹„ìœ¨']
df1['ì–´ë¦°ì´ë¹„ìœ¨'] = df1['ì–´ë¦°ì´ì¸êµ¬'] / df1['ì „ì²´ì¸êµ¬']
df1.info()

df1.describe()

del df1['ì–´ë¦°ì´ì¸êµ¬']
del df1['ì „ì²´ì¸êµ¬']
df1.describe()


X = df1.drop(columns=["ì‚¬ê³ ê±´ìˆ˜"])   # ë…ë¦½ë³€ìˆ˜
y = df1["ì‚¬ê³ ê±´ìˆ˜"]                # ì¢…ì†ë³€ìˆ˜

# í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (80% í•™ìŠµ, 20% í…ŒìŠ¤íŠ¸)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# =============================
# 1. ìŠ¤ì¼€ì¼ë§ (í‘œì¤€í™”)
# =============================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# =============================
# 2. ì„ í˜•íšŒê·€ ëª¨ë¸ í•™ìŠµ
# =============================
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# ì˜ˆì¸¡
y_pred = model.predict(X_test_scaled)

# =============================
# 3. ì„±ëŠ¥ í‰ê°€
# =============================
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print("ğŸ“Š ì„ í˜•íšŒê·€ ë¶„ì„ ê²°ê³¼")
print("RÂ² (ê²°ì •ê³„ìˆ˜):", r2)
print("MSE (í‰ê· ì œê³±ì˜¤ì°¨):", mse)

# =============================
# 4. íšŒê·€ê³„ìˆ˜ í™•ì¸ (ìŠ¤ì¼€ì¼ë§ëœ ê¸°ì¤€)
# =============================
coef_df = pd.DataFrame({
    "ë³€ìˆ˜": X.columns,
    "íšŒê·€ê³„ìˆ˜": model.coef_
}).sort_values(by="íšŒê·€ê³„ìˆ˜", ascending=False)

print("\níšŒê·€ê³„ìˆ˜ (í‘œì¤€í™”ëœ ë°ì´í„° ê¸°ì¤€):")
print(coef_df)

print("\nì ˆí¸(intercept):", model.intercept_)
import statsmodels.api as sm

# ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ ì„¤ì •
X = df1.drop(columns=["ì‚¬ê³ ê±´ìˆ˜"])
y = df1["ì‚¬ê³ ê±´ìˆ˜"]

# ìƒìˆ˜í•­(intercept) ì¶”ê°€
X = sm.add_constant(X)

# OLS íšŒê·€ ëª¨ë¸ ì í•©
model = sm.OLS(y, X).fit()

# ê²°ê³¼ ìš”ì•½ ì¶œë ¥
print(model.summary())

import statsmodels.api as sm
import statsmodels.formula.api as smf

# ë°ì´í„°í”„ë ˆì„ dfì—ì„œ ì¢…ì†ë³€ìˆ˜(y)ì™€ ë…ë¦½ë³€ìˆ˜(X) ì„¤ì •
# ì—¬ê¸°ì„œëŠ” ì „ì²´ ì—´ ì¤‘ "ì‚¬ê³ ê±´ìˆ˜"ë¥¼ ì¢…ì†ë³€ìˆ˜ë¡œ í•˜ê³  ë‚˜ë¨¸ì§€ë¥¼ ë…ë¦½ë³€ìˆ˜ë¡œ ì‚¬ìš©
# (í•„ìš”ì—†ëŠ” ë³€ìˆ˜ëŠ” dropí•˜ì„¸ìš”)

formula = "ì‚¬ê³ ê±´ìˆ˜ ~ êµ¬ì—­ì§€ì •ìˆ˜ + Q('ì‹œì„¤ë¬¼ CCTV ìˆ˜') + Q('ì‹œì„¤ë¬¼ ë„ë¡œí‘œì§€íŒ ìˆ˜') + Q('ì‹œì„¤ë¬¼ ê³¼ì†ë°©ì§€í„± ìˆ˜') + ë³´í˜¸êµ¬ì—­ë„ë¡œí­ + í‰ê· ì£¼í–‰ì†ë„ + ë¶ˆë²•ì£¼ì •ì°¨ìœ„ë°˜ê±´ìˆ˜ + Q('ì‹ í˜¸ë“± 300m') + ì–´ë¦°ì´ë¹„ìœ¨"

# í¬ì•„ì†¡ íšŒê·€ ì í•©
poisson_model = smf.glm(formula=formula, data=df, family=sm.families.Poisson()).fit()

# ê²°ê³¼ ìš”ì•½
print(poisson_model.summary())


import geopandas as gpd
g = gpd.read_file("./data/N3A_G0100000.shp")
print(g.columns)
print(g.head())

# -*- coding: utf-8 -*-
import os
import geopandas as gpd
import folium
from shapely.ops import unary_union

# -----------------------
# ê²½ë¡œ ì„¤ì •
# -----------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SHP_PATH = os.path.join(BASE_DIR, "data", "N3A_G0100000.shp")

# -----------------------
# Shapefile ì½ê¸°
# -----------------------
g = gpd.read_file(SHP_PATH)
g = g.to_crs(4326) if g.crs else g.set_crs(5179, allow_override=True).to_crs(4326)

# âœ… GU ì»¬ëŸ¼ ìƒì„± (ì´ë¦„)
if "NAME" in g.columns:
    g["GU"] = g["NAME"].astype(str).str.strip()
else:
    g["GU"] = g.iloc[:,0].astype(str)

# âœ… ëŒ€êµ¬ 8ê°œ êµ¬/êµ°ë§Œ ì¶”ì¶œ
DIST_LABELS = ["ë¶êµ¬", "ë‚¨êµ¬", "ë™êµ¬", "ì„œêµ¬", "ì¤‘êµ¬", "ìˆ˜ì„±êµ¬", "ë‹¬ì„œêµ¬", "ë‹¬ì„±êµ°"]
dg = g[g["GU"].isin(DIST_LABELS)][["GU","geometry"]].reset_index(drop=True)

# -----------------------
# ì¤‘ì‹¬ ì¢Œí‘œ ê³„ì‚°
# -----------------------
def _centroid_latlon(gdf):
    ua = getattr(gdf.geometry, "union_all", None)
    if callable(ua):
        u = gdf.geometry.union_all()
    else:
        u = unary_union(gdf.geometry.values)
    c = u.centroid
    return float(c.y), float(c.x)

lat, lon = _centroid_latlon(dg)

# -----------------------
# folium ì§€ë„ ìƒì„±
# -----------------------
m = folium.Map(location=[lat, lon], zoom_start=11, tiles="CartoDB positron")

for _, row in dg.iterrows():
    folium.GeoJson(
        row.geometry.__geo_interface__,
        style_function=lambda x: {"color": "blue", "weight": 2, "fillOpacity": 0.1},
        tooltip=row["GU"]
    ).add_to(m)

# -----------------------
# HTML ì €ì¥
# -----------------------
SAVE_PATH = os.path.join(BASE_DIR, "test_map.html")
m.save(SAVE_PATH)

print(f"âœ… ì§€ë„ ì €ì¥ ì™„ë£Œ: {SAVE_PATH}")
print(f"âœ… íŒŒì¼ í¬ê¸°: {os.path.getsize(SAVE_PATH)/1024/1024:.2f} MB")
